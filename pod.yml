How To Configure Etcd Backup on Kubernetes
Taking an etcd backup in a Kubernetes cluster is essential for disaster recovery and ensures that you can restore the state of your cluster in case of a failure. Below are the steps to take a backup of etcd in a Kubernetes cluster:

### Prerequisites
- You have access to the etcd cluster.
- You have `etcdctl` installed on your machine or on the master node of your Kubernetes cluster.
- You have access to the Kubernetes master node where etcd is running.

### Steps to Take an etcd Backup

1. **Access the Kubernetes Master Node**:
   - Log in to the Kubernetes master node where etcd is running

2. **Identify the etcd Pod**:
   - Find the etcd pod running in your cluster.
   kubectl get pods -n kube-system | grep etcd
   
   This command will list all pods in the `kube-system` namespace and filter out the etcd pod. Note the name of the etcd pod (e.g., `etcd-master-node-1`).

3. **Create a Backup Directory**:
   - Create a directory on the master node to store the backup file.
   mkdir -p /path/to/backup/directory
4. **Run the etcd Backup Command**:
   - Use `etcdctl` to create a snapshot of the etcd database. Ensure you have the correct etcd client version that matches your etcd server version.
   ETCDCTL_API=3 etcdctl \
     --endpoints=https://127.0.0.1:2379 \
     --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --cert=/etc/kubernetes/pki/etcd/peer.crt \
     --key=/etc/kubernetes/pki/etcd/peer.key \
     snapshot save /path/to/backup/directory/etcd-snapshot.db
   ```

   - Here is an explanation of the options used:
     - `--endpoints`: The URL of the etcd server.
     - `--cacert`: Path to the CA certificate.
     - `--cert`: Path to the etcd server certificate.
     - `--key`: Path to the etcd server key.
     - `snapshot save`: Command to save the snapshot.
     - `/path/to/backup/directory/etcd-snapshot.db`: Path where the snapshot will be saved.

5. **Verify the Backup**:
   - Check the backup file to ensure it has been created.
   ls -l /path/to/backup/directory/etcd-snapshot.db

   - You can also verify the integrity of the snapshot file.
   ETCDCTL_API=3 etcdctl \
     --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --cert=/etc/kubernetes/pki/etcd/peer.crt \
     --key=/etc/kubernetes/pki/etcd/peer.key \
     snapshot status /path/to/backup/directory/etcd-snapshot.db

6. **Secure the Backup File**:
   - Ensure the backup file is stored securely and consider transferring it to a remote storage location.

 scp /path/to/backup/directory/etcd-snapshot.db user@backup-server:/path/to/remote/backup/directory
   ``
By following these steps, you can ensure regular backups of your etcd database, which is critical for disaster recovery in Kubernetes clusters.

Restoring The Etcd Backup 
Retrieving and restoring an etcd backup in a Kubernetes cluster is a critical step in disaster recovery. Here’s a step-by-step guide on how to retrieve and restore an etcd backup:
Prerequisites
    • Ensure you have the etcd backup file (etcd-snapshot.db).
    • Ensure etcdctl is installed and is the correct version that matches your etcd server
    • Access to the Kubernetes master node.
Steps to Restore etcd from a Backup
    • Access the Kubernetes Master Node:
    • Log in to the Kubernetes master node where etcd is running.

    • Stop the etcd Pod running in namespace (kube-system) :
kubectl get pods -n kube-system
           kubectl delete pod etcd-ip-172-31-12-33 -n kube-system
    • Move or Rename the Existing etcd Data Directory:
    • Move or rename the existing etcd data directory to avoid any conflicts.

        mv /var/lib/etcd /var/lib/etcd.bak
    •  Restore the etcd Snapshot:
          Use etcdctl to restore the snapshot.

 ETCDCTL_API=3 etcdctl snapshot restore         /path/to/local/backup/directory/etcd-snapshot.db \
  --data-dir=/var/lib/etcd

This command restores the backup to the specified data directory (/var/lib/etcd).
    • Check if the etcd Pod in running in kube-system namespace or not 
( Verify The Restore)
         kubectl get pods -n kube-system
      root@ip-172-31-12-33:~# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-74d5f9d7bb-dndbm   1/1     Running   6          13d
calico-node-6fszp                          1/1     Running   6          13d
calico-node-7b5kw                          1/1     Running   6          13d
calico-node-qvtqf                          1/1     Running   6          13d
coredns-76f75df574-p8nmv                   1/1     Running   6          13d
coredns-76f75df574-q6h67                   1/1     Running   6          13d
etcd-ip-172-31-12-33                       1/1     Running   6          26m
kube-apiserver-ip-172-31-12-33             1/1     Running   6          13d
kube-controller-manager-ip-172-31-12-33    1/1     Running   6          13d
kube-proxy-4p8hk                           1/1     Running   6          13d
kube-proxy-gnhjp                           1/1     Running   7          13d
kube-proxy-xdrsc                           1/1     Running   6          13d
kube-scheduler-ip-172-31-12-33             1/1     Running   6          13d
metrics-server-84989b68d9-5tmdz            1/1     Running   3          7d20h

    • Verify that the restored data is present.

ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/peer.crt \
  --key=/etc/kubernetes/pki/etcd/peer.key \
  get / --prefix --keys-only
    • This command lists all the keys to verify the data.


By following these steps, you can successfully restore an etcd backup and ensure the continuity of your Kubernetes cluster operations.


history: --


39  kubectl get pods -n kube-system | grep etcd
   40  mkdir -p /backup/bk1
   41  ETCDCTL_API=3 etcdctl      --endpoints=https://127.0.0.1:2379      --cacert=/etc/kubernetes/pki/etcd/ca.crt      --cert=/etc/kubernetes/pki/etcd/peer.crt      --key=/etc/kubernetes/pki/etcd/peer.key      snapshot save /backup/bk1/etcd-snapshot.db
   42  apt update -y && snap install etcd && apt install etcd-client -y
   43  apt install etcd-client -y
   44  ETCDCTL_API=3 etcdctl      --endpoints=https://127.0.0.1:2379      --cacert=/etc/kubernetes/pki/etcd/ca.crt      --cert=/etc/kubernetes/pki/etcd/peer.crt      --key=/etc/kubernetes/pki/etcd/peer.key      snapshot save /backup/bk1/etcd-snapshot.db
   45  ls -l /backup/bk1/directory/etcd-snapshot.db
   46  ls -l /backup/bk1/etcd-snapshot.db
   47  ETCDCTL_API=3 etcdctl      --cacert=/etc/kubernetes/pki/etcd/ca.crt      --cert=/etc/kubernetes/pki/etcd/peer.crt      --key=/etc/kubernetes/pki/etcd/peer.key      snapshot status /backup/bk1/etcd-snapshot.db
   48  kubectl get pods -n kube-system
   49  kubectl delete pod etcd-ip-172-31-81-190 -n kube-system
   50  kubectl get pods -n kube-system
   51  kubectl delete pod etcd-ip-172-31-81-190 -n kube-system
   52  kubectl get pods -n kube-system
   53  cd /var/lib
   54  ls
   55  mv /var/lib/etcd /var/lib/etcd.bak
   56  ls
   57  ETCDCTL_API=3 etcdctl snapshot restore         /backup/bk1/etcd-snapshot.db   --data-dir=/var/lib/etcd
   58  ls
   59  cd etcd
   60  ls
   61  cd member
   62  ls
   63  kubectl get pods -n kube-system
   64  cd /backup
   65  cd bk1
   66  ls
   67  chmod 777 etcd-snapshot.db
   68  history


